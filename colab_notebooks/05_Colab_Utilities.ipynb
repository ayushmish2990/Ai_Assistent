{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkXxX9TyNkXxX9TyNk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utilities_header"
      },
      "source": [
        "# ðŸ› ï¸ Colab Utilities and Helper Functions\n",
        "\n",
        "## AI Coding Assistant - Utility Functions\n",
        "\n",
        "This notebook contains utility functions and helper tools for managing your AI coding assistant in Google Colab.\n",
        "\n",
        "### ðŸŽ¯ What's Included\n",
        "\n",
        "- **ðŸ”§ Environment Management**: Setup and configuration utilities\n",
        "- **ðŸ“Š Model Management**: Load, save, and manage AI models\n",
        "- **ðŸ”„ Data Sync**: Synchronize data between local and cloud\n",
        "- **ðŸ“± Service Management**: Start, stop, and monitor services\n",
        "- **ðŸ› Debugging Tools**: Logging and error handling utilities\n",
        "- **ðŸ“ˆ Performance Monitoring**: Track resource usage and performance\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_utilities"
      },
      "outputs": [],
      "source": [
        "# ðŸ“¦ Import Required Libraries\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import psutil\n",
        "import logging\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional\n",
        "\n",
        "# Install additional utilities if needed\n",
        "try:\n",
        "    import psutil\n",
        "except ImportError:\n",
        "    !pip install -q psutil\n",
        "    import psutil\n",
        "\n",
        "try:\n",
        "    import GPUtil\n",
        "except ImportError:\n",
        "    !pip install -q GPUtil\n",
        "    import GPUtil\n",
        "\n",
        "print('âœ… Utilities imported successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "environment_utils"
      },
      "source": [
        "## ðŸ”§ Environment Management Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "environment_manager"
      },
      "outputs": [],
      "source": [
        "# ðŸ”§ Environment Management Class\n",
        "\n",
        "class ColabEnvironmentManager:\n",
        "    def __init__(self, project_root: str = '/content/ai_assistant'):\n",
        "        self.project_root = Path(project_root)\n",
        "        self.config_file = self.project_root / 'config' / 'colab_config.json'\n",
        "        self.log_file = self.project_root / 'logs' / 'colab.log'\n",
        "        self.setup_logging()\n",
        "    \n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        os.makedirs(self.log_file.parent, exist_ok=True)\n",
        "        \n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(self.log_file),\n",
        "                logging.StreamHandler(sys.stdout)\n",
        "            ]\n",
        "        )\n",
        "        self.logger = logging.getLogger('ColabEnvironment')\n",
        "    \n",
        "    def get_system_info(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get comprehensive system information\"\"\"\n",
        "        info = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'python_version': sys.version,\n",
        "            'platform': sys.platform,\n",
        "            'cpu_count': psutil.cpu_count(),\n",
        "            'cpu_percent': psutil.cpu_percent(interval=1),\n",
        "            'memory': {\n",
        "                'total': psutil.virtual_memory().total,\n",
        "                'available': psutil.virtual_memory().available,\n",
        "                'percent': psutil.virtual_memory().percent\n",
        "            },\n",
        "            'disk': {\n",
        "                'total': psutil.disk_usage('/').total,\n",
        "                'free': psutil.disk_usage('/').free,\n",
        "                'percent': psutil.disk_usage('/').percent\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # GPU information\n",
        "        try:\n",
        "            gpus = GPUtil.getGPUs()\n",
        "            info['gpu'] = []\n",
        "            for gpu in gpus:\n",
        "                info['gpu'].append({\n",
        "                    'name': gpu.name,\n",
        "                    'memory_total': gpu.memoryTotal,\n",
        "                    'memory_used': gpu.memoryUsed,\n",
        "                    'memory_free': gpu.memoryFree,\n",
        "                    'temperature': gpu.temperature,\n",
        "                    'load': gpu.load\n",
        "                })\n",
        "        except Exception as e:\n",
        "            info['gpu'] = f'GPU info unavailable: {str(e)}'\n",
        "        \n",
        "        return info\n",
        "    \n",
        "    def save_config(self, config: Dict[str, Any]):\n",
        "        \"\"\"Save configuration to file\"\"\"\n",
        "        os.makedirs(self.config_file.parent, exist_ok=True)\n",
        "        with open(self.config_file, 'w') as f:\n",
        "            json.dump(config, f, indent=2)\n",
        "        self.logger.info(f'Configuration saved to {self.config_file}')\n",
        "    \n",
        "    def load_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Load configuration from file\"\"\"\n",
        "        if self.config_file.exists():\n",
        "            with open(self.config_file, 'r') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "    \n",
        "    def create_project_structure(self):\n",
        "        \"\"\"Create complete project directory structure\"\"\"\n",
        "        directories = [\n",
        "            'backend/app/api',\n",
        "            'backend/app/core',\n",
        "            'backend/app/models',\n",
        "            'backend/app/services',\n",
        "            'backend/app/utils',\n",
        "            'frontend/src/components',\n",
        "            'frontend/src/pages',\n",
        "            'frontend/src/utils',\n",
        "            'data/training',\n",
        "            'data/models',\n",
        "            'data/datasets',\n",
        "            'logs',\n",
        "            'config',\n",
        "            'scripts',\n",
        "            'tests'\n",
        "        ]\n",
        "        \n",
        "        for directory in directories:\n",
        "            dir_path = self.project_root / directory\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "            \n",
        "            # Create __init__.py for Python packages\n",
        "            if 'backend' in directory or 'frontend' in directory:\n",
        "                init_file = dir_path / '__init__.py'\n",
        "                if not init_file.exists():\n",
        "                    init_file.touch()\n",
        "        \n",
        "        self.logger.info(f'Project structure created at {self.project_root}')\n",
        "    \n",
        "    def install_requirements(self, requirements: List[str]):\n",
        "        \"\"\"Install Python packages\"\"\"\n",
        "        for package in requirements:\n",
        "            try:\n",
        "                subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', package], \n",
        "                             check=True, capture_output=True, text=True)\n",
        "                self.logger.info(f'âœ… Installed {package}')\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                self.logger.error(f'âŒ Failed to install {package}: {e}')\n",
        "    \n",
        "    def check_gpu_availability(self) -> Dict[str, Any]:\n",
        "        \"\"\"Check GPU availability and status\"\"\"\n",
        "        try:\n",
        "            import torch\n",
        "            gpu_info = {\n",
        "                'torch_cuda_available': torch.cuda.is_available(),\n",
        "                'torch_cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
        "            }\n",
        "            \n",
        "            if torch.cuda.is_available():\n",
        "                gpu_info['current_device'] = torch.cuda.current_device()\n",
        "                gpu_info['device_name'] = torch.cuda.get_device_name()\n",
        "                gpu_info['memory_allocated'] = torch.cuda.memory_allocated()\n",
        "                gpu_info['memory_reserved'] = torch.cuda.memory_reserved()\n",
        "            \n",
        "            return gpu_info\n",
        "        except ImportError:\n",
        "            return {'error': 'PyTorch not available'}\n",
        "\n",
        "# Initialize environment manager\n",
        "env_manager = ColabEnvironmentManager()\n",
        "print('âœ… Environment manager initialized!')\n",
        "\n",
        "# Display system information\n",
        "system_info = env_manager.get_system_info()\n",
        "print('\\nðŸ“Š System Information:')\n",
        "print(f\"CPU: {system_info['cpu_count']} cores, {system_info['cpu_percent']:.1f}% usage\")\n",
        "print(f\"Memory: {system_info['memory']['percent']:.1f}% used\")\n",
        "print(f\"Disk: {system_info['disk']['percent']:.1f}% used\")\n",
        "\n",
        "gpu_info = env_manager.check_gpu_availability()\n",
        "if gpu_info.get('torch_cuda_available'):\n",
        "    print(f\"ðŸš€ GPU Available: {gpu_info['device_name']}\")\n",
        "else:\n",
        "    print('âš ï¸ No GPU available')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_utils"
      },
      "source": [
        "## ðŸ¤– Model Management Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_manager"
      },
      "outputs": [],
      "source": [
        "# ðŸ¤– Model Management Class\n",
        "\n",
        "class ColabModelManager:\n",
        "    def __init__(self, models_dir: str = '/content/ai_assistant/data/models'):\n",
        "        self.models_dir = Path(models_dir)\n",
        "        os.makedirs(self.models_dir, exist_ok=True)\n",
        "        self.logger = logging.getLogger('ModelManager')\n",
        "    \n",
        "    def download_model(self, model_name: str, model_type: str = 'huggingface') -> bool:\n",
        "        \"\"\"Download a model from various sources\"\"\"\n",
        "        try:\n",
        "            if model_type == 'huggingface':\n",
        "                from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "                \n",
        "                self.logger.info(f'Downloading {model_name} from Hugging Face...')\n",
        "                \n",
        "                # Download tokenizer\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "                tokenizer_path = self.models_dir / f'{model_name.replace(\"/\", \"_\")}_tokenizer'\n",
        "                tokenizer.save_pretrained(tokenizer_path)\n",
        "                \n",
        "                # Download model\n",
        "                model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "                model_path = self.models_dir / f'{model_name.replace(\"/\", \"_\")}_model'\n",
        "                model.save_pretrained(model_path)\n",
        "                \n",
        "                self.logger.info(f'âœ… Model {model_name} downloaded successfully')\n",
        "                return True\n",
        "            \n",
        "            else:\n",
        "                self.logger.error(f'Unsupported model type: {model_type}')\n",
        "                return False\n",
        "                \n",
        "        except Exception as e:\n",
        "            self.logger.error(f'Failed to download {model_name}: {str(e)}')\n",
        "            return False\n",
        "    \n",
        "    def list_local_models(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"List all locally stored models\"\"\"\n",
        "        models = []\n",
        "        \n",
        "        for model_dir in self.models_dir.iterdir():\n",
        "            if model_dir.is_dir():\n",
        "                model_info = {\n",
        "                    'name': model_dir.name,\n",
        "                    'path': str(model_dir),\n",
        "                    'size_mb': sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file()) / (1024 * 1024),\n",
        "                    'created': datetime.fromtimestamp(model_dir.stat().st_ctime).isoformat()\n",
        "                }\n",
        "                models.append(model_info)\n",
        "        \n",
        "        return models\n",
        "    \n",
        "    def delete_model(self, model_name: str) -> bool:\n",
        "        \"\"\"Delete a local model\"\"\"\n",
        "        try:\n",
        "            model_path = self.models_dir / model_name\n",
        "            if model_path.exists():\n",
        "                import shutil\n",
        "                shutil.rmtree(model_path)\n",
        "                self.logger.info(f'âœ… Model {model_name} deleted')\n",
        "                return True\n",
        "            else:\n",
        "                self.logger.warning(f'Model {model_name} not found')\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            self.logger.error(f'Failed to delete {model_name}: {str(e)}')\n",
        "            return False\n",
        "    \n",
        "    def get_model_info(self, model_name: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Get detailed information about a model\"\"\"\n",
        "        model_path = self.models_dir / model_name\n",
        "        \n",
        "        if not model_path.exists():\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            # Try to load config if available\n",
        "            config_file = model_path / 'config.json'\n",
        "            config = {}\n",
        "            if config_file.exists():\n",
        "                with open(config_file, 'r') as f:\n",
        "                    config = json.load(f)\n",
        "            \n",
        "            return {\n",
        "                'name': model_name,\n",
        "                'path': str(model_path),\n",
        "                'config': config,\n",
        "                'files': [f.name for f in model_path.iterdir()],\n",
        "                'size_mb': sum(f.stat().st_size for f in model_path.rglob('*') if f.is_file()) / (1024 * 1024)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            self.logger.error(f'Error getting model info: {str(e)}')\n",
        "            return None\n",
        "\n",
        "# Initialize model manager\n",
        "model_manager = ColabModelManager()\n",
        "print('âœ… Model manager initialized!')\n",
        "\n",
        "# List available models\n",
        "local_models = model_manager.list_local_models()\n",
        "print(f'\\nðŸ“¦ Local Models: {len(local_models)} found')\n",
        "for model in local_models:\n",
        "    print(f\"  - {model['name']} ({model['size_mb']:.1f} MB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "service_utils"
      },
      "source": [
        "## ðŸ”„ Service Management Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "service_manager"
      },
      "outputs": [],
      "source": [
        "# ðŸ”„ Service Management Class\n",
        "\n",
        "import threading\n",
        "import queue\n",
        "from contextlib import contextmanager\n",
        "\n",
        "class ColabServiceManager:\n",
        "    def __init__(self):\n",
        "        self.services = {}\n",
        "        self.logger = logging.getLogger('ServiceManager')\n",
        "    \n",
        "    def start_backend(self, port: int = 8000, host: str = '0.0.0.0') -> bool:\n",
        "        \"\"\"Start the FastAPI backend service\"\"\"\n",
        "        try:\n",
        "            import uvicorn\n",
        "            from backend.app.main import app\n",
        "            \n",
        "            # Start in a separate thread\n",
        "            def run_server():\n",
        "                uvicorn.run(app, host=host, port=port, log_level=\"info\")\n",
        "            \n",
        "            backend_thread = threading.Thread(target=run_server, daemon=True)\n",
        "            backend_thread.start()\n",
        "            \n",
        "            self.services['backend'] = {\n",
        "                'thread': backend_thread,\n",
        "                'port': port,\n",
        "                'host': host,\n",
        "                'status': 'running'\n",
        "            }\n",
        "            \n",
        "            self.logger.info(f'âœ… Backend started on {host}:{port}')\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            self.logger.error(f'Failed to start backend: {str(e)}')\n",
        "            return False\n",
        "    \n",
        "    def start_frontend(self, port: int = 8501) -> bool:\n",
        "        \"\"\"Start the Streamlit frontend service\"\"\"\n",
        "        try:\n",
        "            # Create a simple Streamlit app\n",
        "            frontend_code = '''\n",
        "import streamlit as st\n",
        "import requests\n",
        "import json\n",
        "\n",
        "st.set_page_config(page_title=\"AI Coding Assistant\", page_icon=\"ðŸ¤–\")\n",
        "\n",
        "st.title(\"ðŸ¤– AI Coding Assistant\")\n",
        "st.markdown(\"### Powered by Google Colab\")\n",
        "\n",
        "# API endpoint\n",
        "API_BASE = \"http://localhost:8000\"\n",
        "\n",
        "# Chat interface\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"Ask me to generate code...\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    \n",
        "    with st.chat_message(\"assistant\"):\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                f\"{API_BASE}/api/code/generate\",\n",
        "                json={\"prompt\": prompt, \"language\": \"python\"}\n",
        "            )\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                code = result.get(\"code\", \"No code generated\")\n",
        "                st.code(code, language=\"python\")\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": code})\n",
        "            else:\n",
        "                st.error(\"Failed to generate code\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error: {str(e)}\")\n",
        "            '''\n",
        "            \n",
        "            # Save frontend app\n",
        "            frontend_file = '/content/ai_assistant/frontend/streamlit_app.py'\n",
        "            os.makedirs(os.path.dirname(frontend_file), exist_ok=True)\n",
        "            with open(frontend_file, 'w') as f:\n",
        "                f.write(frontend_code.strip())\n",
        "            \n",
        "            # Start Streamlit\n",
        "            def run_streamlit():\n",
        "                os.system(f'streamlit run {frontend_file} --server.port {port} --server.headless true')\n",
        "            \n",
        "            frontend_thread = threading.Thread(target=run_streamlit, daemon=True)\n",
        "            frontend_thread.start()\n",
        "            \n",
        "            self.services['frontend'] = {\n",
        "                'thread': frontend_thread,\n",
        "                'port': port,\n",
        "                'status': 'running'\n",
        "            }\n",
        "            \n",
        "            self.logger.info(f'âœ… Frontend started on port {port}')\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            self.logger.error(f'Failed to start frontend: {str(e)}')\n",
        "            return False\n",
        "    \n",
        "    def setup_ngrok(self, port: int, auth_token: str = None) -> Optional[str]:\n",
        "        \"\"\"Setup ngrok tunnel for external access\"\"\"\n",
        "        try:\n",
        "            from pyngrok import ngrok\n",
        "            \n",
        "            if auth_token:\n",
        "                ngrok.set_auth_token(auth_token)\n",
        "            \n",
        "            # Create tunnel\n",
        "            tunnel = ngrok.connect(port)\n",
        "            public_url = tunnel.public_url\n",
        "            \n",
        "            self.logger.info(f'âœ… Ngrok tunnel created: {public_url}')\n",
        "            return public_url\n",
        "            \n",
        "        except Exception as e:\n",
        "            self.logger.error(f'Failed to setup ngrok: {str(e)}')\n",
        "            return None\n",
        "    \n",
        "    def get_service_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get status of all services\"\"\"\n",
        "        status = {}\n",
        "        \n",
        "        for service_name, service_info in self.services.items():\n",
        "            status[service_name] = {\n",
        "                'status': service_info['status'],\n",
        "                'port': service_info.get('port'),\n",
        "                'thread_alive': service_info['thread'].is_alive() if 'thread' in service_info else False\n",
        "            }\n",
        "        \n",
        "        return status\n",
        "    \n",
        "    def stop_service(self, service_name: str) -> bool:\n",
        "        \"\"\"Stop a specific service\"\"\"\n",
        "        if service_name in self.services:\n",
        "            try:\n",
        "                # Note: In Colab, we can't easily stop threads\n",
        "                # This is a limitation of the environment\n",
        "                self.services[service_name]['status'] = 'stopped'\n",
        "                self.logger.info(f'Service {service_name} marked as stopped')\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                self.logger.error(f'Error stopping {service_name}: {str(e)}')\n",
        "                return False\n",
        "        else:\n",
        "            self.logger.warning(f'Service {service_name} not found')\n",
        "            return False\n",
        "\n",
        "# Initialize service manager\n",
        "service_manager = ColabServiceManager()\n",
        "print('âœ… Service manager initialized!')"
      ]
    }
  ]
}