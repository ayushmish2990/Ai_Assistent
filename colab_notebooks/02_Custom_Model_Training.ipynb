{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# 🧠 Custom AI Model Training Pipeline\n",
        "\n",
        "This notebook provides a complete pipeline for training your own AI coding assistant model.\n",
        "\n",
        "## Training Options:\n",
        "- 🔥 **Fine-tune CodeLlama**: Recommended for coding tasks\n",
        "- 🌟 **Fine-tune StarCoder**: Great for code generation\n",
        "- 🚀 **Fine-tune Llama 2**: General purpose with coding capabilities\n",
        "- 🛠️ **Custom Training**: Train from scratch (advanced)\n",
        "\n",
        "## Features:\n",
        "- ✅ Data collection and preprocessing\n",
        "- ✅ Model fine-tuning with LoRA/QLoRA\n",
        "- ✅ Training monitoring and checkpointing\n",
        "- ✅ Model evaluation and testing\n",
        "- ✅ Integration with your existing API\n",
        "\n",
        "## Requirements:\n",
        "- GPU runtime (T4 minimum, V100/A100 recommended)\n",
        "- Google Drive for data storage\n",
        "- Hugging Face account (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_training_environment"
      },
      "outputs": [],
      "source": [
        "# 📦 Install Training Dependencies\n",
        "\n",
        "# Option 1: Minimal Setup (Recommended for beginners)\n",
        "# Upload requirements_minimal.txt to your Colab session, then run:\n",
        "# !pip install -r requirements_minimal.txt\n",
        "\n",
        "# Option 2: Manual Installation (if you prefer individual packages)\n",
        "!pip install transformers==4.35.0\n",
        "!pip install datasets==2.14.0\n",
        "!pip install peft==0.6.0\n",
        "!pip install bitsandbytes==0.41.1\n",
        "!pip install accelerate==0.24.0\n",
        "!pip install wandb\n",
        "!pip install torch==2.1.0\n",
        "!pip install trl==0.7.4\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up project directories\n",
        "project_path = '/content/drive/MyDrive/ai-coding-assistant'\n",
        "training_path = f'{project_path}/training'\n",
        "data_path = f'{training_path}/data'\n",
        "models_path = f'{training_path}/models'\n",
        "logs_path = f'{training_path}/logs'\n",
        "\n",
        "for path in [training_path, data_path, models_path, logs_path]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "print(f'✅ Training environment setup complete')\n",
        "print(f'🔥 GPU Available: {torch.cuda.is_available()}')\n",
        "print(f'💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB' if torch.cuda.is_available() else 'No GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_collection"
      },
      "outputs": [],
      "source": [
        "# 📊 Data Collection and Preparation\n",
        "\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "import requests\n",
        "from typing import List, Dict\n",
        "\n",
        "class CodeDataCollector:\n",
        "    def __init__(self, data_path: str):\n",
        "        self.data_path = data_path\n",
        "        self.datasets = []\n",
        "    \n",
        "    def collect_github_data(self, languages: List[str] = ['python', 'javascript', 'java']):\n",
        "        \"\"\"Collect code data from GitHub repositories\"\"\"\n",
        "        print(\"📥 Collecting GitHub data...\")\n",
        "        \n",
        "        # Use the Stack dataset (filtered GitHub code)\n",
        "        for lang in languages:\n",
        "            try:\n",
        "                print(f\"  Downloading {lang} code samples...\")\n",
        "                dataset = load_dataset(\n",
        "                    \"bigcode/the-stack-dedup\",\n",
        "                    data_dir=f\"data/{lang}\",\n",
        "                    split=\"train\",\n",
        "                    streaming=True\n",
        "                )\n",
        "                \n",
        "                # Take first 1000 samples for demo\n",
        "                samples = []\n",
        "                for i, sample in enumerate(dataset):\n",
        "                    if i >= 1000:\n",
        "                        break\n",
        "                    samples.append({\n",
        "                        'content': sample['content'],\n",
        "                        'language': lang,\n",
        "                        'source': 'github'\n",
        "                    })\n",
        "                \n",
        "                self.datasets.extend(samples)\n",
        "                print(f\"  ✅ Collected {len(samples)} {lang} samples\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Error collecting {lang} data: {e}\")\n",
        "    \n",
        "    def collect_stackoverflow_data(self):\n",
        "        \"\"\"Collect Q&A data from Stack Overflow\"\"\"\n",
        "        print(\"📥 Collecting Stack Overflow data...\")\n",
        "        \n",
        "        try:\n",
        "            # Use a curated dataset of Stack Overflow questions\n",
        "            dataset = load_dataset(\"koutch/stackoverflow_python\", split=\"train\")\n",
        "            \n",
        "            # Take first 500 samples\n",
        "            for i, sample in enumerate(dataset):\n",
        "                if i >= 500:\n",
        "                    break\n",
        "                \n",
        "                self.datasets.append({\n",
        "                    'content': f\"Question: {sample['question']}\\n\\nAnswer: {sample['answer']}\",\n",
        "                    'language': 'python',\n",
        "                    'source': 'stackoverflow'\n",
        "                })\n",
        "            \n",
        "            print(f\"  ✅ Collected {min(500, len(dataset))} Stack Overflow samples\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error collecting Stack Overflow data: {e}\")\n",
        "    \n",
        "    def create_instruction_dataset(self):\n",
        "        \"\"\"Convert collected data to instruction format\"\"\"\n",
        "        print(\"🔄 Converting to instruction format...\")\n",
        "        \n",
        "        instruction_data = []\n",
        "        \n",
        "        for sample in self.datasets:\n",
        "            if sample['source'] == 'github':\n",
        "                # Create code explanation tasks\n",
        "                instruction_data.append({\n",
        "                    'instruction': f\"Explain this {sample['language']} code:\",\n",
        "                    'input': sample['content'][:500],  # Truncate for training\n",
        "                    'output': f\"This {sample['language']} code demonstrates programming concepts and functionality.\",\n",
        "                    'language': sample['language']\n",
        "                })\n",
        "                \n",
        "                # Create code completion tasks\n",
        "                if len(sample['content']) > 100:\n",
        "                    split_point = len(sample['content']) // 2\n",
        "                    instruction_data.append({\n",
        "                        'instruction': f\"Complete this {sample['language']} code:\",\n",
        "                        'input': sample['content'][:split_point],\n",
        "                        'output': sample['content'][split_point:split_point+200],\n",
        "                        'language': sample['language']\n",
        "                    })\n",
        "            \n",
        "            elif sample['source'] == 'stackoverflow':\n",
        "                instruction_data.append({\n",
        "                    'instruction': \"Answer this programming question:\",\n",
        "                    'input': sample['content'],\n",
        "                    'output': \"Here's a solution to your programming question.\",\n",
        "                    'language': sample['language']\n",
        "                })\n",
        "        \n",
        "        return instruction_data\n",
        "    \n",
        "    def save_dataset(self, instruction_data: List[Dict]):\n",
        "        \"\"\"Save the prepared dataset\"\"\"\n",
        "        dataset_file = f'{self.data_path}/training_dataset.json'\n",
        "        \n",
        "        with open(dataset_file, 'w') as f:\n",
        "            json.dump(instruction_data, f, indent=2)\n",
        "        \n",
        "        print(f\"💾 Dataset saved to {dataset_file}\")\n",
        "        print(f\"📊 Total samples: {len(instruction_data)}\")\n",
        "        \n",
        "        return dataset_file\n",
        "\n",
        "# Initialize data collector\n",
        "collector = CodeDataCollector(data_path)\n",
        "print('✅ Data collector initialized')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "collect_training_data"
      },
      "outputs": [],
      "source": [
        "# 🚀 Start Data Collection\n",
        "\n",
        "# Collect data from various sources\n",
        "collector.collect_github_data(['python', 'javascript'])  # Start with 2 languages\n",
        "collector.collect_stackoverflow_data()\n",
        "\n",
        "# Convert to instruction format\n",
        "instruction_data = collector.create_instruction_dataset()\n",
        "\n",
        "# Save the dataset\n",
        "dataset_file = collector.save_dataset(instruction_data)\n",
        "\n",
        "print(f'\\n🎉 Data collection complete!')\n",
        "print(f'📁 Dataset location: {dataset_file}')\n",
        "print(f'📊 Total training samples: {len(instruction_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_model_training"
      },
      "outputs": [],
      "source": [
        "# 🤖 Setup Model Training\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    TrainingArguments, Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model_name: str = \"codellama/CodeLlama-7b-hf\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.training_args = None\n",
        "    \n",
        "    def load_model_and_tokenizer(self):\n",
        "        \"\"\"Load the base model and tokenizer\"\"\"\n",
        "        print(f\"📥 Loading model: {self.model_name}\")\n",
        "        \n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        \n",
        "        # Load model with 4-bit quantization for memory efficiency\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        \n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True\n",
        "        )\n",
        "        \n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        \n",
        "        print(\"✅ Model and tokenizer loaded\")\n",
        "    \n",
        "    def setup_lora(self):\n",
        "        \"\"\"Setup LoRA for efficient fine-tuning\"\"\"\n",
        "        print(\"🔧 Setting up LoRA configuration...\")\n",
        "        \n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            r=16,  # Rank\n",
        "            lora_alpha=32,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "            bias=\"none\"\n",
        "        )\n",
        "        \n",
        "        self.model = get_peft_model(self.model, lora_config)\n",
        "        self.model.print_trainable_parameters()\n",
        "        \n",
        "        print(\"✅ LoRA setup complete\")\n",
        "    \n",
        "    def prepare_dataset(self, dataset_file: str):\n",
        "        \"\"\"Prepare the dataset for training\"\"\"\n",
        "        print(\"📊 Preparing dataset...\")\n",
        "        \n",
        "        # Load the dataset\n",
        "        with open(dataset_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        # Format for training\n",
        "        formatted_data = []\n",
        "        for sample in data:\n",
        "            text = f\"### Instruction: {sample['instruction']}\\n### Input: {sample['input']}\\n### Output: {sample['output']}\"\n",
        "            formatted_data.append({'text': text})\n",
        "        \n",
        "        # Create dataset\n",
        "        dataset = Dataset.from_list(formatted_data)\n",
        "        \n",
        "        # Tokenize\n",
        "        def tokenize_function(examples):\n",
        "            return self.tokenizer(\n",
        "                examples['text'],\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=512\n",
        "            )\n",
        "        \n",
        "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "        \n",
        "        # Split into train/validation\n",
        "        train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
        "        \n",
        "        print(f\"✅ Dataset prepared - Train: {len(train_test_split['train'])}, Val: {len(train_test_split['test'])}\")\n",
        "        \n",
        "        return train_test_split\n",
        "    \n",
        "    def setup_training_args(self):\n",
        "        \"\"\"Setup training arguments\"\"\"\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=f'{models_path}/checkpoints',\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=4,\n",
        "            per_device_eval_batch_size=4,\n",
        "            gradient_accumulation_steps=4,\n",
        "            warmup_steps=100,\n",
        "            learning_rate=2e-4,\n",
        "            fp16=True,\n",
        "            logging_steps=10,\n",
        "            evaluation_strategy=\"steps\",\n",
        "            eval_steps=100,\n",
        "            save_steps=500,\n",
        "            save_total_limit=3,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            report_to=\"none\"  # Disable wandb for now\n",
        "        )\n",
        "        \n",
        "        print(\"✅ Training arguments configured\")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = ModelTrainer(\"codellama/CodeLlama-7b-hf\")\n",
        "print('✅ Model trainer initialized')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_training"
      },
      "outputs": [],
      "source": [
        "# 🚀 Start Model Training\n",
        "\n",
        "# Load model and tokenizer\n",
        "trainer.load_model_and_tokenizer()\n",
        "\n",
        "# Setup LoRA\n",
        "trainer.setup_lora()\n",
        "\n",
        "# Prepare dataset\n",
        "train_dataset = trainer.prepare_dataset(dataset_file)\n",
        "\n",
        "# Setup training arguments\n",
        "trainer.setup_training_args()\n",
        "\n",
        "# Create data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=trainer.tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Create Trainer\n",
        "huggingface_trainer = Trainer(\n",
        "    model=trainer.model,\n",
        "    args=trainer.training_args,\n",
        "    train_dataset=train_dataset['train'],\n",
        "    eval_dataset=train_dataset['test'],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=trainer.tokenizer\n",
        ")\n",
        "\n",
        "print(\"🎯 Starting training...\")\n",
        "print(\"⏱️  This will take several hours depending on your GPU\")\n",
        "\n",
        "# Start training\n",
        "huggingface_trainer.train()\n",
        "\n",
        "print(\"🎉 Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_model"
      },
      "outputs": [],
      "source": [
        "# 💾 Save the Trained Model\n",
        "\n",
        "# Save the model\n",
        "model_save_path = f'{models_path}/fine_tuned_codellama'\n",
        "\n",
        "# Save the LoRA adapter\n",
        "trainer.model.save_pretrained(model_save_path)\n",
        "trainer.tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"✅ Model saved to: {model_save_path}\")\n",
        "\n",
        "# Save training metadata\n",
        "metadata = {\n",
        "    'base_model': trainer.model_name,\n",
        "    'training_samples': len(instruction_data),\n",
        "    'training_epochs': trainer.training_args.num_train_epochs,\n",
        "    'learning_rate': trainer.training_args.learning_rate,\n",
        "    'model_path': model_save_path,\n",
        "    'training_date': pd.Timestamp.now().isoformat()\n",
        "}\n",
        "\n",
        "with open(f'{model_save_path}/training_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"📋 Training metadata saved\")\n",
        "print(\"\\n🎊 Your custom AI coding assistant model is ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_model"
      },
      "outputs": [],
      "source": [
        "# 🧪 Test the Trained Model\n",
        "\n",
        "from peft import PeftModel\n",
        "\n",
        "def test_model(prompt: str, max_length: int = 200):\n",
        "    \"\"\"Test the trained model with a prompt\"\"\"\n",
        "    \n",
        "    # Format the prompt\n",
        "    formatted_prompt = f\"### Instruction: {prompt}\\n### Input: \\n### Output: \"\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = trainer.tokenizer(formatted_prompt, return_tensors=\"pt\").to(trainer.model.device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = trainer.model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=trainer.tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    response = trainer.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract the generated part\n",
        "    generated = response[len(formatted_prompt):].strip()\n",
        "    \n",
        "    return generated\n",
        "\n",
        "# Test with various prompts\n",
        "test_prompts = [\n",
        "    \"Write a Python function to calculate factorial\",\n",
        "    \"Create a JavaScript function to reverse a string\",\n",
        "    \"Explain what this code does: for i in range(10): print(i)\",\n",
        "    \"Complete this Python code: def fibonacci(n):\"\n",
        "]\n",
        "\n",
        "print(\"🧪 Testing the trained model...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\nTest {i}: {prompt}\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    try:\n",
        "        response = test_model(prompt)\n",
        "        print(f\"Response: {response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    \n",
        "    print(\"=\" * 50)\n",
        "\n",
        "print(\"✅ Model testing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_inference_service"
      },
      "outputs": [],
      "source": [
        "# 🔧 Create Inference Service for Integration\n",
        "\n",
        "inference_service_code = '''\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CustomModelService:\n",
        "    def __init__(self, model_path: str, base_model_name: str = \"codellama/CodeLlama-7b-hf\"):\n",
        "        self.model_path = model_path\n",
        "        self.base_model_name = base_model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.load_model()\n",
        "    \n",
        "    def load_model(self):\n",
        "        \"\"\"Load the fine-tuned model\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Loading custom model from {self.model_path}\")\n",
        "            \n",
        "            # Load tokenizer\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            \n",
        "            # Load base model\n",
        "            base_model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.base_model_name,\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "            \n",
        "            # Load LoRA adapter\n",
        "            self.model = PeftModel.from_pretrained(base_model, self.model_path)\n",
        "            \n",
        "            logger.info(\"Custom model loaded successfully\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading custom model: {e}\")\n",
        "            raise\n",
        "    \n",
        "    async def generate_code(self, prompt: str, language: str = \"python\") -> str:\n",
        "        \"\"\"Generate code using the custom model\"\"\"\n",
        "        try:\n",
        "            formatted_prompt = f\"### Instruction: Generate {language} code for: {prompt}\\n### Input: \\n### Output: \"\n",
        "            \n",
        "            inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=512,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id\n",
        "                )\n",
        "            \n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            generated = response[len(formatted_prompt):].strip()\n",
        "            \n",
        "            return generated\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating code: {e}\")\n",
        "            return f\"Error generating code: {str(e)}\"\n",
        "    \n",
        "    async def suggest_completion(self, code: str, cursor_position: int) -> str:\n",
        "        \"\"\"Suggest code completion\"\"\"\n",
        "        try:\n",
        "            formatted_prompt = f\"### Instruction: Complete this code\\n### Input: {code}\\n### Output: \"\n",
        "            \n",
        "            inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=256,\n",
        "                    temperature=0.3,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id\n",
        "                )\n",
        "            \n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            generated = response[len(formatted_prompt):].strip()\n",
        "            \n",
        "            return generated\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error suggesting completion: {e}\")\n",
        "            return f\"Error: {str(e)}\"\n",
        "    \n",
        "    async def chat_with_context(self, message: str, context=None) -> str:\n",
        "        \"\"\"Chat with context using the custom model\"\"\"\n",
        "        try:\n",
        "            formatted_prompt = f\"### Instruction: Answer this programming question\\n### Input: {message}\\n### Output: \"\n",
        "            \n",
        "            inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=512,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id\n",
        "                )\n",
        "            \n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            generated = response[len(formatted_prompt):].strip()\n",
        "            \n",
        "            return generated\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in chat: {e}\")\n",
        "            return f\"Error: {str(e)}\"\n",
        "'''\n",
        "\n",
        "# Save the inference service\n",
        "with open(f'{models_path}/custom_model_service.py', 'w') as f:\n",
        "    f.write(inference_service_code.strip())\n",
        "\n",
        "print(f\"✅ Custom model inference service created at: {models_path}/custom_model_service.py\")\n",
        "print(\"\\n🔧 Integration Instructions:\")\n",
        "print(\"1. Copy custom_model_service.py to your backend/app/services/ directory\")\n",
        "print(\"2. Update your AI service to use the custom model\")\n",
        "print(\"3. Ensure the model files are accessible from your backend\")\n",
        "print(\"4. Test the integration with your existing API endpoints\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_complete"
      },
      "source": [
        "## 🎉 Training Complete!\n",
        "\n",
        "Your custom AI coding assistant model has been successfully trained and is ready for use.\n",
        "\n",
        "### 📊 What You've Accomplished:\n",
        "\n",
        "1. **Data Collection**: Gathered coding data from GitHub and Stack Overflow\n",
        "2. **Data Preprocessing**: Converted raw data to instruction format\n",
        "3. **Model Fine-tuning**: Fine-tuned CodeLlama with LoRA for efficiency\n",
        "4. **Model Testing**: Validated the model's performance\n",
        "5. **Integration Service**: Created inference service for your API\n",
        "\n",
        "### 🔧 Next Steps:\n",
        "\n",
        "#### Option 1: Use in Colab\n",
        "- Continue using the model directly in this Colab environment\n",
        "- Perfect for experimentation and testing\n",
        "\n",
        "#### Option 2: Download and Deploy Locally\n",
        "- Download the model files from Google Drive\n",
        "- Set up local inference server\n",
        "- Integrate with your existing backend\n",
        "\n",
        "#### Option 3: Hybrid Approach\n",
        "- Use custom model for specific tasks\n",
        "- Fall back to OpenAI API when needed\n",
        "- Best of both worlds\n",
        "\n",
        "### 📁 Files Created:\n",
        "\n",
        "- `training_dataset.json`: Your training data\n",
        "- `fine_tuned_codellama/`: Your trained model\n",
        "- `custom_model_service.py`: Integration service\n",
        "- `training_metadata.json`: Training information\n",
        "\n",
        "### 🚀 Performance Tips:\n",
        "\n",
        "1. **More Data**: Collect more domain-specific data for better performance\n",
        "2. **Longer Training**: Increase epochs for better convergence\n",
        "3. **Hyperparameter Tuning**: Experiment with learning rates and batch sizes\n",
        "4. **Model Selection**: Try different base models (StarCoder, Llama 2, etc.)\n",
        "\n",
        "### 💡 Advanced Features:\n",
        "\n",
        "- **Multi-language Support**: Train on multiple programming languages\n",
        "- **Code Review**: Fine-tune for code review and bug detection\n",
        "- **Documentation**: Train for automatic documentation generation\n",
        "- **Testing**: Generate unit tests automatically\n",
        "\n",
        "**Congratulations! You now have your own AI coding assistant! 🎊**\n"
      ]
    }
  ]
}