{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# 🔄 Hybrid AI Assistant Configuration\n",
        "\n",
        "This notebook sets up a hybrid approach where you can:\n",
        "- Keep your **frontend and API** running locally\n",
        "- Use **Colab for model training** and heavy computations\n",
        "- **Seamlessly switch** between local and cloud inference\n",
        "- **Sync data and models** between environments\n",
        "\n",
        "## 🏗️ Architecture Overview\n",
        "\n",
        "```\n",
        "Local Environment          │  Google Colab\n",
        "                          │\n",
        "┌─────────────────────┐    │  ┌─────────────────────┐\n",
        "│   Frontend (React)  │    │  │  Model Training     │\n",
        "│   - UI Components   │    │  │  - Data Processing  │\n",
        "│   - User Interface  │    │  │  - Fine-tuning      │\n",
        "└─────────────────────┘    │  │  - Evaluation       │\n",
        "           │               │  └─────────────────────┘\n",
        "┌─────────────────────┐    │  ┌─────────────────────┐\n",
        "│   Backend API       │◄───┼──┤  Inference Service  │\n",
        "│   - FastAPI         │    │  │  - Model Serving    │\n",
        "│   - Local Models    │    │  │  - API Endpoints    │\n",
        "│   - File System     │    │  │  - ngrok Tunnel     │\n",
        "└─────────────────────┘    │  └─────────────────────┘\n",
        "           │               │           │\n",
        "┌─────────────────────┐    │  ┌─────────────────────┐\n",
        "│   Google Drive      │◄───┼──┤  Model Storage      │\n",
        "│   - Model Sync      │    │  │  - Trained Models   │\n",
        "│   - Data Backup     │    │  │  - Checkpoints      │\n",
        "└─────────────────────┘    │  └─────────────────────┘\n",
        "```\n",
        "\n",
        "## ✨ Benefits\n",
        "\n",
        "- 🏠 **Local Development**: Keep your familiar development environment\n",
        "- ☁️ **Cloud Training**: Leverage free GPU/TPU for model training\n",
        "- 🔄 **Flexible Switching**: Use local or cloud models as needed\n",
        "- 💰 **Cost Effective**: Only use cloud resources when necessary\n",
        "- 🔒 **Data Control**: Keep sensitive data local when needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_hybrid_environment"
      },
      "outputs": [],
      "source": [
        "# 🔧 Setup Hybrid Environment\n",
        "\n",
        "!pip install fastapi uvicorn requests python-multipart\n",
        "!pip install google-colab-tools\n",
        "\n",
        "from google.colab import drive, files\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Setup project paths\n",
        "project_path = '/content/drive/MyDrive/ai-coding-assistant'\n",
        "hybrid_path = f'{project_path}/hybrid'\n",
        "config_path = f'{hybrid_path}/config'\n",
        "models_path = f'{hybrid_path}/models'\n",
        "sync_path = f'{hybrid_path}/sync'\n",
        "\n",
        "for path in [hybrid_path, config_path, models_path, sync_path]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "print('✅ Hybrid environment setup complete')\n",
        "print(f'📁 Project path: {project_path}')\n",
        "print(f'🔄 Hybrid path: {hybrid_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_hybrid_config"
      },
      "outputs": [],
      "source": [
        "# ⚙️ Create Hybrid Configuration\n",
        "\n",
        "hybrid_config = {\n",
        "    \"environment\": {\n",
        "        \"local\": {\n",
        "            \"frontend_url\": \"http://localhost:3000\",\n",
        "            \"backend_url\": \"http://localhost:8000\",\n",
        "            \"models_path\": \"./models\",\n",
        "            \"data_path\": \"./data\"\n",
        "        },\n",
        "        \"colab\": {\n",
        "            \"inference_url\": \"\",  # Will be set when ngrok starts\n",
        "            \"models_path\": \"/content/drive/MyDrive/ai-coding-assistant/hybrid/models\",\n",
        "            \"data_path\": \"/content/drive/MyDrive/ai-coding-assistant/hybrid/data\",\n",
        "            \"gpu_available\": True\n",
        "        }\n",
        "    },\n",
        "    \"models\": {\n",
        "        \"primary\": \"local\",  # local, colab, or openai\n",
        "        \"fallback\": \"openai\",\n",
        "        \"local_models\": [\n",
        "            {\n",
        "                \"name\": \"custom_codellama\",\n",
        "                \"path\": \"./models/fine_tuned_codellama\",\n",
        "                \"type\": \"local\",\n",
        "                \"capabilities\": [\"code_generation\", \"completion\", \"chat\"]\n",
        "            }\n",
        "        ],\n",
        "        \"colab_models\": [\n",
        "            {\n",
        "                \"name\": \"colab_codellama\",\n",
        "                \"path\": \"/content/drive/MyDrive/ai-coding-assistant/hybrid/models/fine_tuned_codellama\",\n",
        "                \"type\": \"colab\",\n",
        "                \"capabilities\": [\"code_generation\", \"completion\", \"chat\", \"training\"]\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"sync\": {\n",
        "        \"auto_sync\": True,\n",
        "        \"sync_models\": True,\n",
        "        \"sync_data\": True,\n",
        "        \"sync_interval\": 3600,  # 1 hour\n",
        "        \"compression\": True\n",
        "    },\n",
        "    \"features\": {\n",
        "        \"load_balancing\": True,\n",
        "        \"health_checks\": True,\n",
        "        \"automatic_fallback\": True,\n",
        "        \"performance_monitoring\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save configuration\n",
        "config_file = f'{config_path}/hybrid_config.json'\n",
        "with open(config_file, 'w') as f:\n",
        "    json.dump(hybrid_config, f, indent=2)\n",
        "\n",
        "print(f'✅ Hybrid configuration saved to: {config_file}')\n",
        "print('📋 Configuration overview:')\n",
        "print(f'  - Primary model source: {hybrid_config[\"models\"][\"primary\"]}')\n",
        "print(f'  - Fallback: {hybrid_config[\"models\"][\"fallback\"]}')\n",
        "print(f'  - Auto sync: {hybrid_config[\"sync\"][\"auto_sync\"]}')\n",
        "print(f'  - Load balancing: {hybrid_config[\"features\"][\"load_balancing\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_colab_inference_service"
      },
      "outputs": [],
      "source": [
        "# 🚀 Create Colab Inference Service\n",
        "\n",
        "colab_service_code = '''\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import json\n",
        "import logging\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "app = FastAPI(title=\"Colab AI Inference Service\", version=\"1.0.0\")\n",
        "\n",
        "# Request models\n",
        "class CodeGenerationRequest(BaseModel):\n",
        "    prompt: str\n",
        "    language: str = \"python\"\n",
        "    max_length: int = 512\n",
        "    temperature: float = 0.7\n",
        "\n",
        "class CompletionRequest(BaseModel):\n",
        "    code: str\n",
        "    cursor_position: int\n",
        "    max_length: int = 256\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    message: str\n",
        "    context: Optional[str] = None\n",
        "    max_length: int = 512\n",
        "\n",
        "# Global model variables\n",
        "model = None\n",
        "tokenizer = None\n",
        "model_loaded = False\n",
        "\n",
        "def load_model(model_path: str, base_model_name: str = \"codellama/CodeLlama-7b-hf\"):\n",
        "    global model, tokenizer, model_loaded\n",
        "    \n",
        "    try:\n",
        "        logger.info(f\"Loading model from {model_path}\")\n",
        "        \n",
        "        # Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        \n",
        "        # Load base model\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        \n",
        "        # Load LoRA adapter\n",
        "        model = PeftModel.from_pretrained(base_model, model_path)\n",
        "        model_loaded = True\n",
        "        \n",
        "        logger.info(\"Model loaded successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model: {e}\")\n",
        "        model_loaded = False\n",
        "        raise\n",
        "\n",
        "def generate_text(prompt: str, max_length: int = 512, temperature: float = 0.7) -> str:\n",
        "    if not model_loaded:\n",
        "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
        "    \n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated = response[len(prompt):].strip()\n",
        "        \n",
        "        return generated\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating text: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\n",
        "        \"service\": \"Colab AI Inference Service\",\n",
        "        \"status\": \"running\",\n",
        "        \"model_loaded\": model_loaded,\n",
        "        \"gpu_available\": torch.cuda.is_available()\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"model_loaded\": model_loaded,\n",
        "        \"gpu_memory\": torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
        "    }\n",
        "\n",
        "@app.post(\"/generate-code\")\n",
        "async def generate_code(request: CodeGenerationRequest):\n",
        "    formatted_prompt = f\"### Instruction: Generate {request.language} code for: {request.prompt}\\n### Input: \\n### Output: \"\n",
        "    \n",
        "    generated = generate_text(\n",
        "        formatted_prompt,\n",
        "        max_length=request.max_length,\n",
        "        temperature=request.temperature\n",
        "    )\n",
        "    \n",
        "    return {\"generated_code\": generated}\n",
        "\n",
        "@app.post(\"/suggest-completion\")\n",
        "async def suggest_completion(request: CompletionRequest):\n",
        "    formatted_prompt = f\"### Instruction: Complete this code\\n### Input: {request.code}\\n### Output: \"\n",
        "    \n",
        "    generated = generate_text(\n",
        "        formatted_prompt,\n",
        "        max_length=request.max_length,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    \n",
        "    return {\"completion\": generated}\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat(request: ChatRequest):\n",
        "    formatted_prompt = f\"### Instruction: Answer this programming question\\n### Input: {request.message}\\n### Output: \"\n",
        "    \n",
        "    generated = generate_text(\n",
        "        formatted_prompt,\n",
        "        max_length=request.max_length,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    return {\"response\": generated}\n",
        "\n",
        "@app.post(\"/load-model\")\n",
        "async def load_model_endpoint(model_path: str, base_model: str = \"codellama/CodeLlama-7b-hf\"):\n",
        "    try:\n",
        "        load_model(model_path, base_model)\n",
        "        return {\"status\": \"success\", \"message\": \"Model loaded successfully\"}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)\n",
        "'''\n",
        "\n",
        "# Save the Colab inference service\n",
        "service_file = f'{hybrid_path}/colab_inference_service.py'\n",
        "with open(service_file, 'w') as f:\n",
        "    f.write(colab_service_code.strip())\n",
        "\n",
        "print(f'✅ Colab inference service created: {service_file}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_local_hybrid_service"
      },
      "outputs": [],
      "source": [
        "# 🏠 Create Local Hybrid Service\n",
        "\n",
        "local_hybrid_service = '''\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import json\n",
        "import logging\n",
        "from typing import Optional, Dict, Any, List\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class HybridAIService:\n",
        "    def __init__(self, config_path: str = \"./hybrid/config/hybrid_config.json\"):\n",
        "        self.config_path = config_path\n",
        "        self.config = self.load_config()\n",
        "        self.local_service = None\n",
        "        self.colab_url = None\n",
        "        self.health_status = {\n",
        "            \"local\": False,\n",
        "            \"colab\": False,\n",
        "            \"openai\": True  # Assume OpenAI is available\n",
        "        }\n",
        "        \n",
        "        # Initialize local service if available\n",
        "        self.init_local_service()\n",
        "    \n",
        "    def load_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Load hybrid configuration\"\"\"\n",
        "        try:\n",
        "            with open(self.config_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading config: {e}\")\n",
        "            return self.get_default_config()\n",
        "    \n",
        "    def get_default_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get default configuration\"\"\"\n",
        "        return {\n",
        "            \"models\": {\n",
        "                \"primary\": \"openai\",\n",
        "                \"fallback\": \"openai\"\n",
        "            },\n",
        "            \"features\": {\n",
        "                \"automatic_fallback\": True,\n",
        "                \"health_checks\": True\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def init_local_service(self):\n",
        "        \"\"\"Initialize local model service if available\"\"\"\n",
        "        try:\n",
        "            # Try to import and initialize local model service\n",
        "            from .custom_model_service import CustomModelService\n",
        "            \n",
        "            local_models = self.config.get(\"models\", {}).get(\"local_models\", [])\n",
        "            if local_models:\n",
        "                model_path = local_models[0][\"path\"]\n",
        "                if Path(model_path).exists():\n",
        "                    self.local_service = CustomModelService(model_path)\n",
        "                    self.health_status[\"local\"] = True\n",
        "                    logger.info(\"Local model service initialized\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Local model service not available: {e}\")\n",
        "            self.health_status[\"local\"] = False\n",
        "    \n",
        "    async def check_colab_health(self) -> bool:\n",
        "        \"\"\"Check if Colab service is available\"\"\"\n",
        "        if not self.colab_url:\n",
        "            return False\n",
        "        \n",
        "        try:\n",
        "            async with aiohttp.ClientSession() as session:\n",
        "                async with session.get(f\"{self.colab_url}/health\", timeout=5) as response:\n",
        "                    if response.status == 200:\n",
        "                        data = await response.json()\n",
        "                        return data.get(\"status\") == \"healthy\"\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Colab health check failed: {e}\")\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    async def update_health_status(self):\n",
        "        \"\"\"Update health status of all services\"\"\"\n",
        "        if self.config.get(\"features\", {}).get(\"health_checks\", True):\n",
        "            self.health_status[\"colab\"] = await self.check_colab_health()\n",
        "    \n",
        "    def get_best_service(self, task_type: str = \"general\") -> str:\n",
        "        \"\"\"Determine the best service to use based on availability and configuration\"\"\"\n",
        "        primary = self.config.get(\"models\", {}).get(\"primary\", \"openai\")\n",
        "        fallback = self.config.get(\"models\", {}).get(\"fallback\", \"openai\")\n",
        "        \n",
        "        # Check primary service\n",
        "        if primary == \"local\" and self.health_status[\"local\"]:\n",
        "            return \"local\"\n",
        "        elif primary == \"colab\" and self.health_status[\"colab\"]:\n",
        "            return \"colab\"\n",
        "        elif primary == \"openai\":\n",
        "            return \"openai\"\n",
        "        \n",
        "        # Fallback to available service\n",
        "        if self.config.get(\"features\", {}).get(\"automatic_fallback\", True):\n",
        "            if fallback == \"local\" and self.health_status[\"local\"]:\n",
        "                return \"local\"\n",
        "            elif fallback == \"colab\" and self.health_status[\"colab\"]:\n",
        "                return \"colab\"\n",
        "            else:\n",
        "                return \"openai\"\n",
        "        \n",
        "        return \"openai\"  # Final fallback\n",
        "    \n",
        "    async def generate_code(self, prompt: str, language: str = \"python\") -> str:\n",
        "        \"\"\"Generate code using the best available service\"\"\"\n",
        "        await self.update_health_status()\n",
        "        service = self.get_best_service(\"code_generation\")\n",
        "        \n",
        "        logger.info(f\"Using {service} service for code generation\")\n",
        "        \n",
        "        try:\n",
        "            if service == \"local\" and self.local_service:\n",
        "                return await self.local_service.generate_code(prompt, language)\n",
        "            \n",
        "            elif service == \"colab\" and self.colab_url:\n",
        "                return await self._call_colab_service(\"/generate-code\", {\n",
        "                    \"prompt\": prompt,\n",
        "                    \"language\": language\n",
        "                })\n",
        "            \n",
        "            else:\n",
        "                # Fallback to OpenAI\n",
        "                return await self._call_openai_service(\"generate_code\", prompt, language)\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in {service} service: {e}\")\n",
        "            # Try fallback\n",
        "            if service != \"openai\":\n",
        "                return await self._call_openai_service(\"generate_code\", prompt, language)\n",
        "            raise\n",
        "    \n",
        "    async def suggest_completion(self, code: str, cursor_position: int) -> str:\n",
        "        \"\"\"Suggest code completion using the best available service\"\"\"\n",
        "        await self.update_health_status()\n",
        "        service = self.get_best_service(\"completion\")\n",
        "        \n",
        "        logger.info(f\"Using {service} service for code completion\")\n",
        "        \n",
        "        try:\n",
        "            if service == \"local\" and self.local_service:\n",
        "                return await self.local_service.suggest_completion(code, cursor_position)\n",
        "            \n",
        "            elif service == \"colab\" and self.colab_url:\n",
        "                return await self._call_colab_service(\"/suggest-completion\", {\n",
        "                    \"code\": code,\n",
        "                    \"cursor_position\": cursor_position\n",
        "                })\n",
        "            \n",
        "            else:\n",
        "                return await self._call_openai_service(\"suggest_completion\", code, cursor_position)\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in {service} service: {e}\")\n",
        "            if service != \"openai\":\n",
        "                return await self._call_openai_service(\"suggest_completion\", code, cursor_position)\n",
        "            raise\n",
        "    \n",
        "    async def chat_with_context(self, message: str, context=None) -> str:\n",
        "        \"\"\"Chat with context using the best available service\"\"\"\n",
        "        await self.update_health_status()\n",
        "        service = self.get_best_service(\"chat\")\n",
        "        \n",
        "        logger.info(f\"Using {service} service for chat\")\n",
        "        \n",
        "        try:\n",
        "            if service == \"local\" and self.local_service:\n",
        "                return await self.local_service.chat_with_context(message, context)\n",
        "            \n",
        "            elif service == \"colab\" and self.colab_url:\n",
        "                return await self._call_colab_service(\"/chat\", {\n",
        "                    \"message\": message,\n",
        "                    \"context\": context\n",
        "                })\n",
        "            \n",
        "            else:\n",
        "                return await self._call_openai_service(\"chat_with_context\", message, context)\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in {service} service: {e}\")\n",
        "            if service != \"openai\":\n",
        "                return await self._call_openai_service(\"chat_with_context\", message, context)\n",
        "            raise\n",
        "    \n",
        "    async def _call_colab_service(self, endpoint: str, data: Dict[str, Any]) -> str:\n",
        "        \"\"\"Call Colab inference service\"\"\"\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.post(f\"{self.colab_url}{endpoint}\", json=data) as response:\n",
        "                if response.status == 200:\n",
        "                    result = await response.json()\n",
        "                    return result.get(\"generated_code\") or result.get(\"completion\") or result.get(\"response\", \"\")\n",
        "                else:\n",
        "                    raise Exception(f\"Colab service error: {response.status}\")\n",
        "    \n",
        "    async def _call_openai_service(self, method: str, *args, **kwargs) -> str:\n",
        "        \"\"\"Fallback to OpenAI service\"\"\"\n",
        "        # Import your existing OpenAI service\n",
        "        from .ai_service import AIService\n",
        "        \n",
        "        ai_service = AIService()\n",
        "        \n",
        "        if method == \"generate_code\":\n",
        "            return await ai_service.generate_code(*args, **kwargs)\n",
        "        elif method == \"suggest_completion\":\n",
        "            return await ai_service.suggest_completion(*args, **kwargs)\n",
        "        elif method == \"chat_with_context\":\n",
        "            return await ai_service.chat_with_context(*args, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown method: {method}\")\n",
        "    \n",
        "    def set_colab_url(self, url: str):\n",
        "        \"\"\"Set the Colab service URL\"\"\"\n",
        "        self.colab_url = url\n",
        "        logger.info(f\"Colab URL set to: {url}\")\n",
        "    \n",
        "    def get_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get current status of all services\"\"\"\n",
        "        return {\n",
        "            \"health_status\": self.health_status,\n",
        "            \"primary_service\": self.config.get(\"models\", {}).get(\"primary\", \"openai\"),\n",
        "            \"colab_url\": self.colab_url,\n",
        "            \"config_loaded\": bool(self.config)\n",
        "        }\n",
        "'''\n",
        "\n",
        "# Save the local hybrid service\n",
        "local_service_file = f'{hybrid_path}/hybrid_ai_service.py'\n",
        "with open(local_service_file, 'w') as f:\n",
        "    f.write(local_hybrid_service.strip())\n",
        "\n",
        "print(f'✅ Local hybrid service created: {local_service_file}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_ngrok_tunnel"
      },
      "outputs": [],
      "source": [
        "# 🌐 Setup ngrok Tunnel for Colab Service\n",
        "\n",
        "!pip install pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import uvicorn\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Set ngrok auth token\n",
        "ngrok.set_auth_token(\"32lxmVvlcVwe0aIQldrGTv9sB0c_2kNsr5fcetExZj6St12dZ\")\n",
        "\n",
        "# Add the hybrid path to Python path\n",
        "sys.path.append(hybrid_path)\n",
        "\n",
        "def start_inference_service():\n",
        "    \"\"\"Start the inference service in a separate thread\"\"\"\n",
        "    # Import the service\n",
        "    from colab_inference_service import app, load_model\n",
        "    \n",
        "    # Load a model if available\n",
        "    model_path = f'{models_path}/fine_tuned_codellama'\n",
        "    if os.path.exists(model_path):\n",
        "        try:\n",
        "            load_model(model_path)\n",
        "            print(f'✅ Model loaded from {model_path}')\n",
        "        except Exception as e:\n",
        "            print(f'⚠️ Could not load model: {e}')\n",
        "    else:\n",
        "        print('⚠️ No trained model found. Service will run without model.')\n",
        "    \n",
        "    # Start the service\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8080, log_level=\"info\")\n",
        "\n",
        "def setup_ngrok_tunnel():\n",
        "    \"\"\"Setup ngrok tunnel for the inference service\"\"\"\n",
        "    try:\n",
        "        # Kill any existing ngrok processes\n",
        "        ngrok.kill()\n",
        "        \n",
        "        # Create tunnel\n",
        "        public_url = ngrok.connect(8080)\n",
        "        print(f'🌐 ngrok tunnel created: {public_url}')\n",
        "        \n",
        "        # Update configuration with the public URL\n",
        "        hybrid_config['environment']['colab']['inference_url'] = str(public_url)\n",
        "        \n",
        "        with open(config_file, 'w') as f:\n",
        "            json.dump(hybrid_config, f, indent=2)\n",
        "        \n",
        "        print(f'✅ Configuration updated with ngrok URL')\n",
        "        print(f'🔗 Your Colab AI service is available at: {public_url}')\n",
        "        \n",
        "        return str(public_url)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f'❌ Error setting up ngrok: {e}')\n",
        "        return None\n",
        "\n",
        "# Start the inference service in background\n",
        "print('🚀 Starting Colab inference service...')\n",
        "service_thread = threading.Thread(target=start_inference_service, daemon=True)\n",
        "service_thread.start()\n",
        "\n",
        "# Wait a moment for the service to start\n",
        "import time\n",
        "time.sleep(5)\n",
        "\n",
        "# Setup ngrok tunnel\n",
        "public_url = setup_ngrok_tunnel()\n",
        "\n",
        "if public_url:\n",
        "    print('\\n🎉 Hybrid setup complete!')\n",
        "    print(f'📡 Colab service URL: {public_url}')\n",
        "    print('📋 Next steps:')\n",
        "    print('  1. Copy the hybrid configuration to your local project')\n",
        "    print('  2. Update your local AI service to use HybridAIService')\n",
        "    print('  3. Set the Colab URL in your local configuration')\n",
        "    print('  4. Test the hybrid functionality')\n",
        "else:\n",
        "    print('❌ Failed to setup ngrok tunnel. Service running locally only.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_sync_utilities"
      },
      "outputs": [],
      "source": [
        "# 🔄 Create Sync Utilities\n",
        "\n",
        "sync_utilities = '''\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import zipfile\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class HybridSyncManager:\n",
        "    def __init__(self, config_path: str):\n",
        "        self.config_path = config_path\n",
        "        self.config = self.load_config()\n",
        "        self.sync_log = []\n",
        "    \n",
        "    def load_config(self) -> Dict:\n",
        "        with open(self.config_path, 'r') as f:\n",
        "            return json.load(f)\n",
        "    \n",
        "    def sync_models_to_local(self, colab_models_path: str, local_models_path: str):\n",
        "        \"\"\"Sync models from Colab to local environment\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Syncing models from {colab_models_path} to {local_models_path}\")\n",
        "            \n",
        "            # Create local models directory\n",
        "            os.makedirs(local_models_path, exist_ok=True)\n",
        "            \n",
        "            # Copy model files\n",
        "            if os.path.exists(colab_models_path):\n",
        "                for item in os.listdir(colab_models_path):\n",
        "                    src = os.path.join(colab_models_path, item)\n",
        "                    dst = os.path.join(local_models_path, item)\n",
        "                    \n",
        "                    if os.path.isdir(src):\n",
        "                        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "                    else:\n",
        "                        shutil.copy2(src, dst)\n",
        "                \n",
        "                self.log_sync(\"models\", \"colab_to_local\", \"success\")\n",
        "                logger.info(\"Models synced successfully\")\n",
        "            else:\n",
        "                logger.warning(f\"Colab models path not found: {colab_models_path}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error syncing models: {e}\")\n",
        "            self.log_sync(\"models\", \"colab_to_local\", \"error\", str(e))\n",
        "    \n",
        "    def sync_data_to_colab(self, local_data_path: str, colab_data_path: str):\n",
        "        \"\"\"Sync data from local to Colab environment\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Syncing data from {local_data_path} to {colab_data_path}\")\n",
        "            \n",
        "            # Create colab data directory\n",
        "            os.makedirs(colab_data_path, exist_ok=True)\n",
        "            \n",
        "            # Copy data files\n",
        "            if os.path.exists(local_data_path):\n",
        "                for item in os.listdir(local_data_path):\n",
        "                    src = os.path.join(local_data_path, item)\n",
        "                    dst = os.path.join(colab_data_path, item)\n",
        "                    \n",
        "                    if os.path.isdir(src):\n",
        "                        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "                    else:\n",
        "                        shutil.copy2(src, dst)\n",
        "                \n",
        "                self.log_sync(\"data\", \"local_to_colab\", \"success\")\n",
        "                logger.info(\"Data synced successfully\")\n",
        "            else:\n",
        "                logger.warning(f\"Local data path not found: {local_data_path}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error syncing data: {e}\")\n",
        "            self.log_sync(\"data\", \"local_to_colab\", \"error\", str(e))\n",
        "    \n",
        "    def create_backup(self, source_path: str, backup_path: str, compress: bool = True):\n",
        "        \"\"\"Create backup of models or data\"\"\"\n",
        "        try:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            \n",
        "            if compress:\n",
        "                backup_file = f\"{backup_path}/backup_{timestamp}.zip\"\n",
        "                os.makedirs(os.path.dirname(backup_file), exist_ok=True)\n",
        "                \n",
        "                with zipfile.ZipFile(backup_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "                    for root, dirs, files in os.walk(source_path):\n",
        "                        for file in files:\n",
        "                            file_path = os.path.join(root, file)\n",
        "                            arcname = os.path.relpath(file_path, source_path)\n",
        "                            zipf.write(file_path, arcname)\n",
        "                \n",
        "                logger.info(f\"Backup created: {backup_file}\")\n",
        "                return backup_file\n",
        "            else:\n",
        "                backup_dir = f\"{backup_path}/backup_{timestamp}\"\n",
        "                shutil.copytree(source_path, backup_dir)\n",
        "                logger.info(f\"Backup created: {backup_dir}\")\n",
        "                return backup_dir\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating backup: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def log_sync(self, sync_type: str, direction: str, status: str, error: str = None):\n",
        "        \"\"\"Log sync operation\"\"\"\n",
        "        log_entry = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"type\": sync_type,\n",
        "            \"direction\": direction,\n",
        "            \"status\": status,\n",
        "            \"error\": error\n",
        "        }\n",
        "        \n",
        "        self.sync_log.append(log_entry)\n",
        "    \n",
        "    def get_sync_history(self) -> List[Dict]:\n",
        "        \"\"\"Get sync history\"\"\"\n",
        "        return self.sync_log\n",
        "    \n",
        "    def save_sync_log(self, log_path: str):\n",
        "        \"\"\"Save sync log to file\"\"\"\n",
        "        with open(log_path, 'w') as f:\n",
        "            json.dump(self.sync_log, f, indent=2)\n",
        "\n",
        "# Utility functions for easy use\n",
        "def quick_sync_models(config_path: str):\n",
        "    \"\"\"Quick function to sync models from Colab to local\"\"\"\n",
        "    sync_manager = HybridSyncManager(config_path)\n",
        "    config = sync_manager.config\n",
        "    \n",
        "    colab_path = config['environment']['colab']['models_path']\n",
        "    local_path = config['environment']['local']['models_path']\n",
        "    \n",
        "    sync_manager.sync_models_to_local(colab_path, local_path)\n",
        "    return sync_manager.get_sync_history()\n",
        "\n",
        "def quick_sync_data(config_path: str):\n",
        "    \"\"\"Quick function to sync data from local to Colab\"\"\"\n",
        "    sync_manager = HybridSyncManager(config_path)\n",
        "    config = sync_manager.config\n",
        "    \n",
        "    local_path = config['environment']['local']['data_path']\n",
        "    colab_path = config['environment']['colab']['data_path']\n",
        "    \n",
        "    sync_manager.sync_data_to_colab(local_path, colab_path)\n",
        "    return sync_manager.get_sync_history()\n",
        "'''\n",
        "\n",
        "# Save sync utilities\n",
        "sync_file = f'{hybrid_path}/sync_utilities.py'\n",
        "with open(sync_file, 'w') as f:\n",
        "    f.write(sync_utilities.strip())\n",
        "\n",
        "print(f'✅ Sync utilities created: {sync_file}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_integration_guide"
      },
      "outputs": [],
      "source": [
        "# 📖 Create Integration Guide\n",
        "\n",
        "integration_guide = '''\n",
        "# 🔄 Hybrid AI Assistant Integration Guide\n",
        "\n",
        "This guide helps you integrate the hybrid approach with your existing AI coding assistant.\n",
        "\n",
        "## 📁 File Structure\n",
        "\n",
        "```\n",
        "ai-coding-assistant/\n",
        "├── backend/\n",
        "│   ├── app/\n",
        "│   │   ├── services/\n",
        "│   │   │   ├── ai_service.py          # Original AI service\n",
        "│   │   │   ├── hybrid_ai_service.py   # New hybrid service\n",
        "│   │   │   └── custom_model_service.py # Local model service\n",
        "│   │   └── main.py\n",
        "├── frontend/\n",
        "├── hybrid/\n",
        "│   ├── config/\n",
        "│   │   └── hybrid_config.json         # Hybrid configuration\n",
        "│   ├── models/                        # Local model storage\n",
        "│   ├── colab_inference_service.py     # Colab service\n",
        "│   ├── hybrid_ai_service.py           # Hybrid service\n",
        "│   └── sync_utilities.py              # Sync utilities\n",
        "└── colab_notebooks/\n",
        "    ├── 01_AI_Assistant_Migration.ipynb\n",
        "    ├── 02_Custom_Model_Training.ipynb\n",
        "    └── 03_Hybrid_Configuration.ipynb\n",
        "```\n",
        "\n",
        "## 🔧 Integration Steps\n",
        "\n",
        "### Step 1: Copy Hybrid Files to Local Project\n",
        "\n",
        "1. Download the hybrid folder from Google Drive\n",
        "2. Copy it to your local project root\n",
        "3. Copy `hybrid_ai_service.py` to `backend/app/services/`\n",
        "\n",
        "### Step 2: Update Your Backend Service\n",
        "\n",
        "Replace your existing AI service import in `main.py`:\n",
        "\n",
        "```python\n",
        "# OLD\n",
        "from app.services.ai_service import AIService\n",
        "\n",
        "# NEW\n",
        "from app.services.hybrid_ai_service import HybridAIService\n",
        "\n",
        "# Initialize hybrid service\n",
        "ai_service = HybridAIService()\n",
        "```\n",
        "\n",
        "### Step 3: Configure Colab URL\n",
        "\n",
        "Set the Colab service URL in your backend:\n",
        "\n",
        "```python\n",
        "# Set Colab URL (from ngrok output above)\n",
        "ai_service.set_colab_url('https://your-ngrok-url.ngrok.io')\n",
        "```\n",
        "\n",
        "### Step 4: Test Integration\n",
        "\n",
        "Test your hybrid setup:\n",
        "\n",
        "```python\n",
        "# Test code generation\n",
        "result = await ai_service.generate_code('create a function to sort a list', 'python')\n",
        "print(result)\n",
        "\n",
        "# Check service status\n",
        "status = ai_service.get_status()\n",
        "print(status)\n",
        "```\n",
        "\n",
        "**Congratulations! Your hybrid AI assistant is now configured! 🎉**\n",
        "'''\n",
        "\n",
        "# Save integration guide\n",
        "guide_file = f'{hybrid_path}/INTEGRATION_GUIDE.md'\n",
        "with open(guide_file, 'w') as f:\n",
        "    f.write(integration_guide.strip())\n",
        "\n",
        "print(f'✅ Integration guide created: {guide_file}')\n",
        "print('\\n🎊 Hybrid configuration complete!')\n",
        "print('📋 Summary of created files:')\n",
        "print(f'  - Configuration: {config_file}')\n",
        "print(f'  - Colab Service: {service_file}')\n",
        "print(f'  - Local Service: {local_service_file}')\n",
        "print(f'  - Sync Utilities: {sync_file}')\n",
        "print(f'  - Integration Guide: {guide_file}')\n",
        "print('\\n🚀 Next steps:')\n",
        "print('  1. Follow the integration guide to set up your local environment')\n",
        "print('  2. Test the hybrid functionality')\n",
        "print('  3. Train custom models using the training notebook')\n",
        "print('  4. Enjoy your hybrid AI coding assistant!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hybrid_complete"
      },
      "source": [
        "## 🎉 Hybrid Configuration Complete!\n",
        "\n",
        "Your hybrid AI coding assistant setup is now ready! This configuration allows you to:\n",
        "\n",
        "### ✨ Key Features\n",
        "\n",
        "- **🏠 Local Development**: Keep your frontend and backend running locally\n",
        "- **☁️ Cloud Training**: Use Colab's free GPU for model training\n",
        "- **🔄 Automatic Fallback**: Seamlessly switch between services\n",
        "- **📊 Load Balancing**: Distribute requests across available services\n",
        "- **🔄 Easy Sync**: Keep models and data synchronized\n",
        "\n",
        "### 📁 Files Created\n",
        "\n",
        "1. **hybrid_config.json** - Main configuration file\n",
        "2. **colab_inference_service.py** - Colab inference service\n",
        "3. **hybrid_ai_service.py** - Local hybrid service manager\n",
        "4. **sync_utilities.py** - Data and model synchronization\n",
        "5. **INTEGRATION_GUIDE.md** - Complete integration instructions\n",
        "\n",
        "### 🎯 Next Steps\n",
        "\n",
        "1. **Download Files**: Download the hybrid folder from Google Drive\n",
        "2. **Follow Guide**: Use the integration guide to set up locally\n",
        "3. **Test Setup**: Verify all services work correctly\n",
        "4. **Train Models**: Use the training notebook for custom models\n",
        "\n",
        "### 🚀 Benefits of This Setup\n",
        "\n",
        "- **Cost Effective**: Only pay for cloud resources when needed\n",
        "- **High Performance**: Use the best service for each task\n",
        "- **Reliable**: Multiple fallback options ensure uptime\n",
        "- **Scalable**: Easy to add more services or models\n",
        "- **Flexible**: Switch between local and cloud as needed\n",
        "\n",
        "**Your hybrid AI coding assistant is ready to use! 🎊**\n"
      ]
    }
  ]
}