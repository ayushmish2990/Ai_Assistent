{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkXxX9TyNkXxX9TyNk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab_env_header"
      },
      "source": [
        "# ðŸš€ Complete Colab Development Environment\n",
        "\n",
        "## AI Coding Assistant - Full Cloud Development Setup\n",
        "\n",
        "This notebook provides a complete development environment for your AI coding assistant entirely within Google Colab. No local setup required!\n",
        "\n",
        "### ðŸŽ¯ What This Environment Includes\n",
        "\n",
        "- **ðŸ”§ Full Backend API**: FastAPI server with all endpoints\n",
        "- **ðŸ¤– AI Services**: Multiple AI model integrations\n",
        "- **ðŸ’¾ Database**: SQLite with full schema\n",
        "- **ðŸŒ Web Interface**: Streamlit-based frontend\n",
        "- **ðŸ“Š Model Training**: Custom model fine-tuning capabilities\n",
        "- **ðŸ”„ Real-time Updates**: Live code editing and testing\n",
        "- **ðŸ“± Mobile Access**: Access from any device\n",
        "\n",
        "### ðŸš€ Quick Start\n",
        "\n",
        "1. **Run Setup**: Execute the setup cells below\n",
        "2. **Start Services**: Launch backend and frontend\n",
        "3. **Access Interface**: Use the provided URLs\n",
        "4. **Start Coding**: Begin using your AI assistant!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_environment"
      },
      "outputs": [],
      "source": [
        "# ðŸ”§ Environment Setup\n",
        "print('ðŸš€ Setting up complete Colab development environment...')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Install required packages\n",
        "packages = [\n",
        "    'fastapi',\n",
        "    'uvicorn[standard]',\n",
        "    'streamlit',\n",
        "    'sqlalchemy',\n",
        "    'alembic',\n",
        "    'python-multipart',\n",
        "    'python-jose[cryptography]',\n",
        "    'passlib[bcrypt]',\n",
        "    'transformers',\n",
        "    'torch',\n",
        "    'accelerate',\n",
        "    'datasets',\n",
        "    'peft',\n",
        "    'bitsandbytes',\n",
        "    'openai',\n",
        "    'anthropic',\n",
        "    'requests',\n",
        "    'websockets',\n",
        "    'pyngrok',\n",
        "    'python-dotenv'\n",
        "]\n",
        "\n",
        "print('ðŸ“¦ Installing packages...')\n",
        "for package in packages:\n",
        "    !pip install -q {package}\n",
        "\n",
        "print('âœ… Package installation complete!')\n",
        "\n",
        "# Create project structure\n",
        "project_root = '/content/ai_assistant'\n",
        "os.makedirs(project_root, exist_ok=True)\n",
        "os.chdir(project_root)\n",
        "\n",
        "# Create directory structure\n",
        "directories = [\n",
        "    'backend/app/api',\n",
        "    'backend/app/core',\n",
        "    'backend/app/models',\n",
        "    'backend/app/services',\n",
        "    'backend/app/utils',\n",
        "    'frontend',\n",
        "    'data/training',\n",
        "    'data/models',\n",
        "    'logs',\n",
        "    'config'\n",
        "]\n",
        "\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(f'ðŸ“ Project structure created at: {project_root}')\n",
        "print('ðŸŽ‰ Environment setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "backend_setup"
      },
      "source": [
        "## ðŸ”§ Backend Setup\n",
        "\n",
        "Setting up the complete FastAPI backend with all services and endpoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_backend_core"
      },
      "outputs": [],
      "source": [
        "# ðŸ”§ Backend Core Configuration\n",
        "\n",
        "# Create main.py\n",
        "main_py = '''\n",
        "from fastapi import FastAPI, HTTPException, Depends\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.security import HTTPBearer\n",
        "import uvicorn\n",
        "import os\n",
        "\n",
        "from app.api import chat, code, auth, models\n",
        "from app.core.config import settings\n",
        "from app.core.database import engine, Base\n",
        "\n",
        "# Create database tables\n",
        "Base.metadata.create_all(bind=engine)\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"AI Coding Assistant API\",\n",
        "    description=\"Complete AI-powered coding assistant\",\n",
        "    version=\"2.0.0\"\n",
        ")\n",
        "\n",
        "# CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Include routers\n",
        "app.include_router(auth.router, prefix=\"/api/auth\", tags=[\"auth\"])\n",
        "app.include_router(chat.router, prefix=\"/api/chat\", tags=[\"chat\"])\n",
        "app.include_router(code.router, prefix=\"/api/code\", tags=[\"code\"])\n",
        "app.include_router(models.router, prefix=\"/api/models\", tags=[\"models\"])\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"AI Coding Assistant API - Running in Colab!\"}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"healthy\", \"environment\": \"colab\"}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "'''\n",
        "\n",
        "with open('backend/app/main.py', 'w') as f:\n",
        "    f.write(main_py.strip())\n",
        "\n",
        "# Create __init__.py files\n",
        "init_files = [\n",
        "    'backend/__init__.py',\n",
        "    'backend/app/__init__.py',\n",
        "    'backend/app/api/__init__.py',\n",
        "    'backend/app/core/__init__.py',\n",
        "    'backend/app/models/__init__.py',\n",
        "    'backend/app/services/__init__.py',\n",
        "    'backend/app/utils/__init__.py'\n",
        "]\n",
        "\n",
        "for init_file in init_files:\n",
        "    with open(init_file, 'w') as f:\n",
        "        f.write('')\n",
        "\n",
        "print('âœ… Backend core files created!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_config"
      },
      "outputs": [],
      "source": [
        "# ðŸ”§ Configuration Files\n",
        "\n",
        "# Create config.py\n",
        "config_py = '''\n",
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "class Settings:\n",
        "    # Database\n",
        "    DATABASE_URL: str = \"sqlite:///./ai_assistant.db\"\n",
        "    \n",
        "    # Security\n",
        "    SECRET_KEY: str = \"colab-ai-assistant-secret-key-2024\"\n",
        "    ALGORITHM: str = \"HS256\"\n",
        "    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30\n",
        "    \n",
        "    # AI Services\n",
        "    OPENAI_API_KEY: Optional[str] = os.getenv(\"OPENAI_API_KEY\")\n",
        "    ANTHROPIC_API_KEY: Optional[str] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "    \n",
        "    # Model Settings\n",
        "    DEFAULT_MODEL: str = \"gpt-3.5-turbo\"\n",
        "    LOCAL_MODEL_PATH: str = \"/content/ai_assistant/data/models\"\n",
        "    \n",
        "    # Colab Settings\n",
        "    COLAB_MODE: bool = True\n",
        "    NGROK_TOKEN: Optional[str] = os.getenv(\"NGROK_TOKEN\")\n",
        "\n",
        "settings = Settings()\n",
        "'''\n",
        "\n",
        "with open('backend/app/core/config.py', 'w') as f:\n",
        "    f.write(config_py.strip())\n",
        "\n",
        "# Create database.py\n",
        "database_py = '''\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "from .config import settings\n",
        "\n",
        "engine = create_engine(settings.DATABASE_URL, connect_args={\"check_same_thread\": False})\n",
        "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
        "Base = declarative_base()\n",
        "\n",
        "def get_db():\n",
        "    db = SessionLocal()\n",
        "    try:\n",
        "        yield db\n",
        "    finally:\n",
        "        db.close()\n",
        "'''\n",
        "\n",
        "with open('backend/app/core/database.py', 'w') as f:\n",
        "    f.write(database_py.strip())\n",
        "\n",
        "print('âœ… Configuration files created!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_models"
      },
      "outputs": [],
      "source": [
        "# ðŸ—„ï¸ Database Models\n",
        "\n",
        "# Create user.py\n",
        "user_py = '''\n",
        "from sqlalchemy import Column, Integer, String, DateTime, Boolean\n",
        "from sqlalchemy.sql import func\n",
        "from ..core.database import Base\n",
        "\n",
        "class User(Base):\n",
        "    __tablename__ = \"users\"\n",
        "    \n",
        "    id = Column(Integer, primary_key=True, index=True)\n",
        "    username = Column(String, unique=True, index=True)\n",
        "    email = Column(String, unique=True, index=True)\n",
        "    hashed_password = Column(String)\n",
        "    is_active = Column(Boolean, default=True)\n",
        "    created_at = Column(DateTime(timezone=True), server_default=func.now())\n",
        "'''\n",
        "\n",
        "with open('backend/app/models/user.py', 'w') as f:\n",
        "    f.write(user_py.strip())\n",
        "\n",
        "# Create chat.py\n",
        "chat_py = '''\n",
        "from sqlalchemy import Column, Integer, String, Text, DateTime, ForeignKey\n",
        "from sqlalchemy.sql import func\n",
        "from sqlalchemy.orm import relationship\n",
        "from ..core.database import Base\n",
        "\n",
        "class ChatSession(Base):\n",
        "    __tablename__ = \"chat_sessions\"\n",
        "    \n",
        "    id = Column(Integer, primary_key=True, index=True)\n",
        "    user_id = Column(Integer, ForeignKey(\"users.id\"))\n",
        "    title = Column(String, default=\"New Chat\")\n",
        "    created_at = Column(DateTime(timezone=True), server_default=func.now())\n",
        "    \n",
        "    messages = relationship(\"ChatMessage\", back_populates=\"session\")\n",
        "\n",
        "class ChatMessage(Base):\n",
        "    __tablename__ = \"chat_messages\"\n",
        "    \n",
        "    id = Column(Integer, primary_key=True, index=True)\n",
        "    session_id = Column(Integer, ForeignKey(\"chat_sessions.id\"))\n",
        "    role = Column(String)  # user, assistant, system\n",
        "    content = Column(Text)\n",
        "    timestamp = Column(DateTime(timezone=True), server_default=func.now())\n",
        "    \n",
        "    session = relationship(\"ChatSession\", back_populates=\"messages\")\n",
        "'''\n",
        "\n",
        "with open('backend/app/models/chat.py', 'w') as f:\n",
        "    f.write(chat_py.strip())\n",
        "\n",
        "print('âœ… Database models created!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_ai_service"
      },
      "outputs": [],
      "source": [
        "# ðŸ¤– AI Service Implementation\n",
        "\n",
        "ai_service_py = '''\n",
        "import openai\n",
        "import anthropic\n",
        "from typing import Optional, List, Dict, Any\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import json\n",
        "import os\n",
        "\n",
        "from ..core.config import settings\n",
        "\n",
        "class ColabAIService:\n",
        "    def __init__(self):\n",
        "        self.openai_client = None\n",
        "        self.anthropic_client = None\n",
        "        self.local_model = None\n",
        "        self.local_tokenizer = None\n",
        "        self.setup_clients()\n",
        "    \n",
        "    def setup_clients(self):\n",
        "        \"\"\"Initialize AI service clients\"\"\"\n",
        "        if settings.OPENAI_API_KEY:\n",
        "            self.openai_client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)\n",
        "        \n",
        "        if settings.ANTHROPIC_API_KEY:\n",
        "            self.anthropic_client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)\n",
        "    \n",
        "    async def load_local_model(self, model_name: str = \"microsoft/DialoGPT-medium\"):\n",
        "        \"\"\"Load a local model for inference\"\"\"\n",
        "        try:\n",
        "            print(f\"Loading local model: {model_name}\")\n",
        "            self.local_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.local_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "            \n",
        "            if torch.cuda.is_available():\n",
        "                self.local_model = self.local_model.cuda()\n",
        "                print(\"âœ… Model loaded on GPU\")\n",
        "            else:\n",
        "                print(\"âœ… Model loaded on CPU\")\n",
        "            \n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading local model: {e}\")\n",
        "            return False\n",
        "    \n",
        "    async def generate_code(self, prompt: str, language: str = \"python\", model: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"Generate code using available AI services\"\"\"\n",
        "        try:\n",
        "            # Try OpenAI first\n",
        "            if self.openai_client:\n",
        "                return await self._generate_with_openai(prompt, language, model)\n",
        "            \n",
        "            # Fallback to Anthropic\n",
        "            if self.anthropic_client:\n",
        "                return await self._generate_with_anthropic(prompt, language)\n",
        "            \n",
        "            # Fallback to local model\n",
        "            if self.local_model:\n",
        "                return await self._generate_with_local(prompt, language)\n",
        "            \n",
        "            return {\"error\": \"No AI services available\"}\n",
        "        \n",
        "        except Exception as e:\n",
        "            return {\"error\": f\"Code generation failed: {str(e)}\"}\n",
        "    \n",
        "    async def _generate_with_openai(self, prompt: str, language: str, model: str = None) -> Dict[str, Any]:\n",
        "        model = model or settings.DEFAULT_MODEL\n",
        "        \n",
        "        system_prompt = f\"\"\"You are an expert {language} programmer. Generate clean, efficient, and well-commented code based on the user's request.\"\"\"\n",
        "        \n",
        "        response = self.openai_client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=2000\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"code\": response.choices[0].message.content,\n",
        "            \"model\": model,\n",
        "            \"service\": \"openai\"\n",
        "        }\n",
        "    \n",
        "    async def _generate_with_anthropic(self, prompt: str, language: str) -> Dict[str, Any]:\n",
        "        system_prompt = f\"You are an expert {language} programmer. Generate clean, efficient, and well-commented code.\"\n",
        "        \n",
        "        response = self.anthropic_client.messages.create(\n",
        "            model=\"claude-3-sonnet-20240229\",\n",
        "            max_tokens=2000,\n",
        "            system=system_prompt,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"code\": response.content[0].text,\n",
        "            \"model\": \"claude-3-sonnet\",\n",
        "            \"service\": \"anthropic\"\n",
        "        }\n",
        "    \n",
        "    async def _generate_with_local(self, prompt: str, language: str) -> Dict[str, Any]:\n",
        "        # Simple local generation (placeholder)\n",
        "        inputs = self.local_tokenizer.encode(prompt, return_tensors='pt')\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.local_model.generate(\n",
        "                inputs,\n",
        "                max_length=inputs.shape[1] + 200,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.local_tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        generated_text = self.local_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        code = generated_text[len(prompt):].strip()\n",
        "        \n",
        "        return {\n",
        "            \"code\": code,\n",
        "            \"model\": \"local\",\n",
        "            \"service\": \"local\"\n",
        "        }\n",
        "    \n",
        "    def get_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get service status\"\"\"\n",
        "        return {\n",
        "            \"openai_available\": self.openai_client is not None,\n",
        "            \"anthropic_available\": self.anthropic_client is not None,\n",
        "            \"local_model_loaded\": self.local_model is not None,\n",
        "            \"gpu_available\": torch.cuda.is_available(),\n",
        "            \"environment\": \"colab\"\n",
        "        }\n",
        "'''\n",
        "\n",
        "with open('backend/app/services/ai_service.py', 'w') as f:\n",
        "    f.write(ai_service_py.strip())\n",
        "\n",
        "print('âœ… AI service implementation created!')"
      ]
    }
  ]
}